import pandas as pd
import numpy as np
import os
import itertools
from socket import gethostname
import neptune
import copy
import json
import gc

import torch
import torch.nn as nn
from torch.distributed import is_initialized
import torchvision
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models.detection import FasterRCNN, FasterRCNN_ResNet50_FPN_Weights, roi_heads
import albumentations as A

from src.data.datasets import RoofsDataset
from src.data.utils import collate_fn
from src.engine.train import FasterRCNN_TorchVision_Engine
from src.network import fasterrcnn_resnet_fpn
from src.scheduler import GradualWarmupScheduler
from src.losses import fastrcnn_focal_loss
from src.utils import dict2str, estimate_maximum_batch_size, seed_everything

SEED = 17112000

def main():
    seed_everything(SEED)
    torch.cuda.empty_cache()
    gc.collect()

    world_size = int(os.environ["WORLD_SIZE"])
    global_rank = int(os.environ["SLURM_PROCID"])
    gpus_per_node = int(os.environ["SLURM_NTASKS_PER_NODE"])

    assert gpus_per_node <= torch.cuda.device_count()
    print(f"Hello from rank {global_rank} of {world_size} on {gethostname()} where there are" \
          f" {gpus_per_node} allocated GPUs per node.")

    if world_size > 1:
        print("DDP is using")
        ddp_setup(world_size, global_rank)

        if global_rank == 0: 
            print(f"\nGroup initialized? {is_initialized()}")
            
    else:
        print("DDP is not using")

    local_rank = global_rank - gpus_per_node * (global_rank // gpus_per_node)
    torch.cuda.set_device(local_rank)

    print(f"host: {gethostname()}, rank: {global_rank}, local_rank: {local_rank}")

    parameters = {
        "batch_size"     : 96,
        "n_epochs"       : 400,
        "warmup_epochs"  : 5,
        "init_lr"        : 1e-2,
        "warmup_factor"  : 3, 
        "train_csv"      : "train_bbox_only_nc.csv",
        "valid_csv"      : "valid.csv",
        "in_size"        : (500, 500),
    }

    model = {
        'backbone' : 'resnet50',
        'name'     : 'Faster R-CNN + FPN',
        'loss'     : 'focal Faster R-CNN loss'
    }

    train_split = pd.read_csv(f'data/splits/{parameters["train_csv"]}')
    valid_split = pd.read_csv(f'data/splits/{parameters["valid_csv"]}')

    transforms = [
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.ShiftScaleRotate(p=0.5),
        A.ToGray(p=0.01),
        A.GaussNoise(p=0.2),
        A.OneOf([
            A.MotionBlur(p=0.2),
            A.MedianBlur(blur_limit=3, p=0.1),
            A.Blur(blur_limit=3, p=0.1),
        ], p=0.2),
        A.OneOf([
            A.CLAHE(),
            A.Sharpen(),
            A.Emboss(),
            A.RandomBrightnessContrast(),
        ], p=0.25),
        A.HueSaturationValue(p=0.25),
        A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.25),
    ]

    loaders_params = {
        "root"        : 'data/Images',
        "train_split" : train_split,
        "valid_split" : valid_split,
        "batch_size"  : parameters['batch_size'], 
        "collate_fn"  : collate_fn,
        "transforms"  : transforms
    }
    train_loader, valid_loader = RoofsDataset.get_dataloaders(**loaders_params)

    focal_loss = torch.hub.load('adeelh/pytorch-multi-class-focal-loss', model='FocalLoss', gamma=2)
    roi_heads.fastrcnn_loss = fastrcnn_focal_loss(focal_loss)

    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT
    network = fasterrcnn_resnet_fpn(backbone_name=model['backbone'], weights=weights, pretrained_backbone=True)
    in_features = network.roi_heads.box_predictor.cls_score.in_features
    network.roi_heads.box_predictor = FastRCNNPredictor(in_features, 4)
    network = network.to(local_rank)

    network_parameters = [p for p in network.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(network_parameters, lr=parameters['init_lr']/parameters['warmup_factor'], momentum=0.9, weight_decay=0.0005)
    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, parameters['n_epochs'] - parameters['warmup_epochs'])
    scheduler = GradualWarmupScheduler(optimizer, multiplier=parameters['warmup_factor'], total_epoch=parameters['warmup_epochs'], after_scheduler=scheduler_cosine)

    if parameters['batch_size'] == 'auto':
        parameters['batch_size'] = estimate_maximum_batch_size(network, local_rank, (3,) + parameters['in_size'])
    
    if global_rank == 0:
        logger = neptune.init_run(
            project="GreekAI/ZINDI-arm",
            description='Faster R-CNN + FPN, nms(trash_thr=0.01)', 
            source_files=["src/*.py", "src/**/*.py", "train.py", "launch.sh"],
            # with_id='ARM-10',
            api_token="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNGRhMzc1Yi00YWI2LTQ3NDktODJiMy1lMzM1Y2Y4ZGU1NmYifQ==",
        ) 

        fields = ['parameters', 'model', 'metrics', 'monitoring', 'images', 'snapshots', 'matric']
        for field in fields:
            try:
                del logger[field]
            except neptune.exceptions.MetadataInconsistency:
                pass

        logger['parameters'] = dict2str(copy.deepcopy(parameters))
        logger['model'] = dict2str(copy.deepcopy(model))
    
        logger.wait()

    else:
        logger = None

    agent = FasterRCNN_TorchVision_Engine(local_rank, network, optimizer, None, scheduler, logger)
    agent.train(parameters['n_epochs'], train_loader, valid_loader)
    
    if world_size > 1:
        ddp_destroy()

if __name__ == '__main__':
    main()
import pandas as pd
import numpy as np
import os
import itertools
from socket import gethostname
import neptune
import copy
import json
import gc

import torch
import torch.nn as nn
from torch.distributed import is_initialized
from transformers import DetrForObjectDetection, DetrImageProcessor
# import torchvision
# from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
# from torchvision.models.detection import FasterRCNN, FasterRCNN_ResNet50_FPN_Weights, roi_heads
import albumentations as A

from src.data.datasets import RoofsDataset_DETR
from src.data.utils import collate_fn_detr
from src.engine.train import DETR_Engine
from src.network import fasterrcnn_resnet_fpn
from src.scheduler import GradualWarmupScheduler
from src.losses import fastrcnn_focal_loss
from src.utils import dict2str, estimate_maximum_batch_size, seed_everything

SEED = 17112000
SPLITS_ROOT = 'data/splits'
IMAGES_ROOT = 'data/Images'

def main():
    seed_everything(SEED)
    torch.cuda.empty_cache()
    gc.collect()

    world_size = int(os.environ["WORLD_SIZE"])
    global_rank = int(os.environ["SLURM_PROCID"])
    gpus_per_node = int(os.environ["SLURM_NTASKS_PER_NODE"])

    assert gpus_per_node <= torch.cuda.device_count()
    print(f"Hello from rank {global_rank} of {world_size} on {gethostname()} where there are" \
          f" {gpus_per_node} allocated GPUs per node.")

    if world_size > 1:
        print("DDP is using")
        ddp_setup(world_size, global_rank)

        if global_rank == 0: 
            print(f"\nGroup initialized? {is_initialized()}")
            
    else:
        print("DDP is not using")

    local_rank = global_rank - gpus_per_node * (global_rank // gpus_per_node)
    torch.cuda.set_device(local_rank)

    print(f"host: {gethostname()}, rank: {global_rank}, local_rank: {local_rank}")

    parameters = {
        "batch_size"     : 'auto',
        "n_epochs"       : 200,
        "warmup_epochs"  : 5,
        "init_lr"        : 1e-4,
        "warmup_factor"  : 5, 
        "train_csv"      : "train_bbox_only_nc.csv",
        "valid_csv"      : "valid_bbox_only.csv",
        "in_size"        : (1000, 1000),
    }

    model = {
        'backbone' : 'resnet50',
        'name'     : 'DETR',
        'loss'     : 'DETR loss'
    }

    transforms = [
        A.HorizontalFlip(p=0.5),
        A.VerticalFlip(p=0.5),
        A.ShiftScaleRotate(p=0.5),
        A.ToGray(p=0.01),
        A.GaussNoise(p=0.2),
        A.OneOf([
            A.MotionBlur(p=0.2),
            A.MedianBlur(blur_limit=3, p=0.1),
            A.Blur(blur_limit=3, p=0.1),
        ], p=0.2),
        A.OneOf([
            A.CLAHE(),
            A.Sharpen(),
            A.Emboss(),
            A.RandomBrightnessContrast(),
        ], p=0.25),
        A.HueSaturationValue(p=0.25),
        A.RGBShift(r_shift_limit=30, g_shift_limit=30, b_shift_limit=30, p=0.25),
    ]

    network = DetrForObjectDetection.from_pretrained("facebook/detr-resnet-50", revision="no_timm", num_labels=3, ignore_mismatched_sizes=True)
    network = network.to(local_rank)

    network_parameters = [p for p in network.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(network_parameters, lr=parameters['init_lr']/parameters['warmup_factor'], momentum=0.9, weight_decay=0.0005)
    scheduler_cosine = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, parameters['n_epochs'] - parameters['warmup_epochs'])
    scheduler = GradualWarmupScheduler(optimizer, multiplier=parameters['warmup_factor'], total_epoch=parameters['warmup_epochs'], after_scheduler=scheduler_cosine)

    processor = DetrImageProcessor.from_pretrained("facebook/detr-resnet-50")
    collate_fn = collate_fn_detr(processor)

    if parameters['batch_size'] == 'auto':
        parameters['batch_size'] = estimate_maximum_batch_size(network, local_rank, (3,) + parameters['in_size'])

    train_loader_params = {
        "root"        : IMAGES_ROOT,
        "dataframe"   : pd.read_csv(os.path.join(SPLITS_ROOT, parameters["train_csv"])),
        "phase"       : "train",
        "batch_size"  : parameters['batch_size'], 
        "collate_fn"  : collate_fn,
        "transforms"  : transforms,
        "processor"   : processor
    }
    valid_loader_params = {
        "root"        : IMAGES_ROOT,
        "dataframe"   : pd.read_csv(os.path.join(SPLITS_ROOT, parameters["valid_csv"])),
        "phase"       : "train",
        "batch_size"  : parameters['batch_size'], 
        "collate_fn"  : collate_fn,
        "transforms"  : [],
        "processor"   : processor
    }
    train_loader = RoofsDataset_DETR.get_dataloader(**train_loader_params)
    valid_loader = RoofsDataset_DETR.get_dataloader(**valid_loader_params)
    
    if global_rank == 0:
        logger = neptune.init_run(
            project="GreekAI/ZINDI-arm",
            description='DETR default + augs + custom nms', 
            source_files=["src/*.py", "src/**/*.py", "train.py", "launch.sh"],
            with_id='ARM-13',
            api_token="eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vYXBwLm5lcHR1bmUuYWkiLCJhcGlfdXJsIjoiaHR0cHM6Ly9hcHAubmVwdHVuZS5haSIsImFwaV9rZXkiOiJiNGRhMzc1Yi00YWI2LTQ3NDktODJiMy1lMzM1Y2Y4ZGU1NmYifQ==",
        ) 

        fields = ['parameters', 'model', 'metrics', 'monitoring', 'images', 'snapshots', 'matric']
        for field in fields:
            try:
                del logger[field]
            except neptune.exceptions.MetadataInconsistency:
                pass

        logger['parameters'] = dict2str(copy.deepcopy(parameters))
        logger['model'] = dict2str(copy.deepcopy(model))
    
        logger.wait()

    else:
        logger = None

    agent = DETR_Engine(local_rank, network, optimizer, None, scheduler, processor, logger)
    agent.train(parameters['n_epochs'], train_loader, valid_loader)
    
    if world_size > 1:
        ddp_destroy()

if __name__ == '__main__':
    main()
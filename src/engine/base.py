import os
from abc import abstractmethod

import torch
import torch.nn as nn
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

class BaseEngine:
    def __init__(self, device, model, optimizer, criterion, scheduler, logger=None):
        self.local_rank = device
        self.global_rank = int(os.environ["SLURM_PROCID"])
        self.world_size = int(os.environ["WORLD_SIZE"])
        
        self.optimizer = optimizer
        self.criterion = criterion
        self.scheduler = scheduler
        
        if self.world_size > 1:
            process_group = dist.new_group()
            self.model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model, process_group).to(self.local_rank)
            self.model = DDP(self.model, find_unused_parameters=True, device_ids=[self.local_rank])
        else:
            self.model = model.to(self.local_rank)

        if self.global_rank == 0:
            self._init_enviroment(logger)
            
    def train(self, n_epochs, train_loader, valid_loader) -> None:
        for epoch in range(1, n_epochs+1):
            ## Train
            self.stage = 'train'
            self.model.train()

            self._init_records()
            self._run_epoch(train_loader)
            self._reduce_records()
            self._process_records()
            self._log_scalars()


            ## Validation
            self.stage = 'valid'
            self.model.eval()

            with torch.no_grad():
                self._init_records()
                self._run_epoch(valid_loader)
                self._reduce_records()
                self._process_records()
                self._log_scalars()


            ## Creating snapshots
            if self.global_rank == 0:
                self._save_snapshot("latest.pth")
                
                if self.criteria(self.records[self.main_metric].item(), self.best_metric):
                    self._save_snapshot("best.pth")
                    self.best_metric = self.records[self.main_metric].item()

                    for key in self.records.keys():
                        self.logger[f'metrics/best/{key}'] = self.records[key].item()
                    self.logger['metrics/best/epoch'] = self.current_epoch

                self.current_epoch += 1

    @abstractmethod
    def _run_epoch(self, loader) -> None:
        pass

    def _compute_metrics(self, prediction, target):
        record = {}        
        return record

    def _init_records(self) -> None:
        self.records = {}
        self.records.update({'loss' : torch.tensor(0.0, device=self.local_rank)})

        self.n_samples = torch.tensor(0, device=self.local_rank)

    def _update_records(self, record, n_samples) -> None:
        for key in self.records.keys():
            self.records[key] += record[key]

        self.n_samples += n_samples

    def _reduce_records(self) -> None:
        if self.world_size > 1:
            for key in self.records.keys():
                dist.reduce(self.records[key], dst=0)
            
            dist.reduce(self.n_samples, dst=0)

    def _process_records(self):
        if self.global_rank == 0:        
            for key in self.records.keys():
                self.records[key] = torch.nan_to_num(self.records[key] / self.n_samples, 0)
        
    def _log_scalars(self) -> None:
        if self.global_rank == 0:
            for key in self.records.keys():
                self.logger[f'metrics/{self.stage}/{key}'].append(self.records[key].item())
            
            if self.stage == 'valid':
                self.logger['metrics/lr'].append(self.optimizer.param_groups[0]['lr'])

    @abstractmethod
    def _log_images(self) -> None:
        pass

    def _load_snapshot(self, snapshot_path):
        snapshot = torch.load(snapshot_path, map_location=f"cuda:{self.local_rank}")

        if self.world_size > 1:
            self.model.module.load_state_dict(snapshot["PARAMS"])
        else: 
            self.model.load_state_dict(snapshot["PARAMS"])
        if not self.scheduler is None and not snapshot.get("SCHEDULER") is None:
            self.scheduler.load_state_dict(snapshot["SCHEDULER"])
        self.optimizer.load_state_dict(snapshot["OPTIMIZER"])
        self.current_epoch = snapshot["CURRENT_EPOCH"]

        print(f"Checkpoint loaded from {snapshot_path}")

    def _save_snapshot(self, snapshot_name):
        snapshot_path = os.path.join(self.snapshots_root, snapshot_name)
        snapshot = dict()
        
        snapshot["PARAMS"] = self.model.module.state_dict() if self.world_size > 1 else self.model.state_dict()
        snapshot["OPTIMIZER"] = self.optimizer.state_dict()
        snapshot["CURRENT_EPOCH"] = self.current_epoch
        snapshot["SCHEDULER"] = self.scheduler.state_dict() if not self.scheduler is None else None

        torch.save(snapshot, snapshot_path)
        print(f"Epoch {self.current_epoch} | Training snapshot saved at {snapshot_path}")

    def _init_enviroment(self, logger):
        self.logger = logger

        self.snapshots_root = os.path.join('exp', self.logger['sys/id'].fetch())
        if not os.path.exists('exp'):
            os.mkdir('exp')
        if not os.path.exists(self.snapshots_root):
            os.mkdir(self.snapshots_root)

        # Init counters

        self.current_epoch = 1

        self.criteria = lambda x, y: x < y
        self.main_metric = None
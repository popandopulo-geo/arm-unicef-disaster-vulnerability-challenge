import os
import abc

import pandas as pd
import numpy as np
import rasterio

import torch
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import albumentations as A
from albumentations.pytorch import ToTensorV2

class BaseDataset(Dataset):
    def __init__(self, root, dataframe, **kwargs):
        self.root = root
        self.dataframe = dataframe

        self.mandatory_transforms = [
            A.Normalize(
                mean=kwargs.get("mean", (0.485, 0.456, 0.406)), 
                std =kwargs.get("sdt" , (0.229, 0.224, 0.225)),
                max_pixel_value=255.0)
        ]

    @abc.abstractmethod
    def __getitem__(self, idx):
        pass

    def __len__(self):
        return self.dataframe.shape[0]

    @classmethod
    def get_dataloaders(cls, root, train_split, valid_split, **kwargs):
        train_dataset = cls(root, train_split, **kwargs)
        valid_dataset = cls(root, valid_split, **kwargs)
        
        if kwargs.get('cpu', False):
            _get_dataloaders = cls.__get_dataloaders_cpu
        elif int(os.environ["WORLD_SIZE"]) > 1: 
            _get_dataloaders = cls.__get_dataloaders_ddp
        else:
            _get_dataloaders = cls.__get_dataloaders
        
        return _get_dataloaders(train_dataset, valid_dataset, kwargs.get('batch_size', 1))

    @staticmethod
    def __get_dataloaders_cpu(train_dataset, valid_dataset, batch_size):
        train_dataloader = DataLoader(train_dataset,
                                      batch_size=batch_size,
                                      shuffle=True,
                                      num_workers=1)

        valid_dataloader = DataLoader(valid_dataset,
                                      batch_size=batch_size,
                                      num_workers=1)
        
        return train_dataloader, valid_dataloader
    
    @staticmethod
    def __get_dataloaders(train_dataset, valid_dataset, batch_size):
        train_dataloader = DataLoader(train_dataset,
                                      batch_size=batch_size,
                                      shuffle=True,
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]))

        valid_dataloader = DataLoader(valid_dataset,
                                      batch_size=batch_size,
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]))
        
        return train_dataloader, valid_dataloader
    
    @staticmethod
    def __get_dataloaders_ddp(train_dataset, valid_dataset, batch_size):
        train_dataloader = DataLoader(train_dataset,
                                      batch_size=batch_size,
                                      shuffle=False,
                                      sampler=DistributedSampler(train_dataset,
                                                                 num_replicas=int(os.environ["WORLD_SIZE"]),
                                                                 rank=int(os.environ["SLURM_PROCID"])),
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]))

        valid_dataloader = DataLoader(valid_dataset,
                                      batch_size=batch_size,
                                      sampler=DistributedSampler(valid_dataset,
                                                                 num_replicas=int(os.environ["WORLD_SIZE"]),
                                                                 rank=int(os.environ["SLURM_PROCID"])),
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]))
        
        return train_dataloader, valid_dataloader
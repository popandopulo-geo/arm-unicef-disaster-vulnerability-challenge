import os
import abc

import pandas as pd
import numpy as np
import cv2

import torch
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
import albumentations as A

class BaseDataset(Dataset):
    def __init__(self, root, dataframe, **kwargs):
        self.root = root
        self.dataframe = dataframe

        self.norm_transform = A.Compose([
            A.Normalize(mean=kwargs.get("mean", (0.485, 0.456, 0.406)), 
                        std =kwargs.get("sdt" , (0.229, 0.224, 0.225)),
                        max_pixel_value=255.0)
        ])

    @abc.abstractmethod
    def __getitem__(self, idx):
        pass

    def __len__(self):
        return self.dataframe.shape[0]

    @classmethod
    def get_dataloaders(cls, root, train_split, valid_split, **kwargs):
        train_dataset = cls(root, train_split, transforms=kwargs.get("transforms", []))
        valid_dataset = cls(root, valid_split, transforms=[])
        
        if kwargs.get('cpu', False):
            _get_dataloaders = cls.__get_dataloaders_cpu
        elif int(os.environ["WORLD_SIZE"]) > 1: 
            _get_dataloaders = cls.__get_dataloaders_ddp
        else:
            _get_dataloaders = cls.__get_dataloaders
        
        return _get_dataloaders(train_dataset, valid_dataset, **kwargs)

    @staticmethod
    def __get_dataloaders_cpu(train_dataset, valid_dataset, **kwargs):
        train_dataloader = DataLoader(train_dataset,
                                      batch_size=kwargs.get("batch_size", 1),
                                      shuffle=True,
                                      num_workers=1,
                                      collate_fn=kwargs.get("collate_fn", None))

        valid_dataloader = DataLoader(valid_dataset,
                                      batch_size=kwargs.get("batch_size", 1),
                                      num_workers=1,
                                      collate_fn=kwargs.get("collate_fn", None))
        
        return train_dataloader, valid_dataloader
    
    @staticmethod
    def __get_dataloaders(train_dataset, valid_dataset, **kwargs):
        train_dataloader = DataLoader(train_dataset,
                                      batch_size=kwargs.get("batch_size", 1),
                                      shuffle=True,
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]),
                                      collate_fn=kwargs.get("collate_fn", None))

        valid_dataloader = DataLoader(valid_dataset,
                                      batch_size=kwargs.get("batch_size", 1),
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]),
                                      collate_fn=kwargs.get("collate_fn", None))
        
        return train_dataloader, valid_dataloader
    
    @staticmethod
    def __get_dataloaders_ddp(train_dataset, valid_dataset, **kwargs):
        train_dataloader = DataLoader(train_dataset,
                                      batch_size=kwargs.get("batch_size", 1),
                                      shuffle=False,
                                      sampler=DistributedSampler(train_dataset,
                                                                 num_replicas=int(os.environ["WORLD_SIZE"]),
                                                                 rank=int(os.environ["SLURM_PROCID"])),
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]),
                                      collate_fn=kwargs.get("collate_fn", None))

        valid_dataloader = DataLoader(valid_dataset,
                                      batch_size=kwargs.get("batch_size", 1),
                                      sampler=DistributedSampler(valid_dataset,
                                                                 num_replicas=int(os.environ["WORLD_SIZE"]),
                                                                 rank=int(os.environ["SLURM_PROCID"])),
                                      num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]),
                                      collate_fn=kwargs.get("collate_fn", None))
        
        return train_dataloader, valid_dataloader

class RoofsDataset(BaseDataset):
    def __init__(self, root, dataframe, **kwargs):
        super(RoofsDataset, self).__init__(root, dataframe, **kwargs)

        self.transforms = kwargs.get("transforms", [])
        self.transforms = A.Compose(self.transforms, bbox_params=A.BboxParams(format='coco', label_fields=['categories'], min_visibility=0.3), additional_targets = {'rgb' : 'mask'})
        self.crop_transform = A.Compose([A.RandomCrop(height=500, width=500, p=1)], bbox_params=A.BboxParams(format='coco', label_fields=['categories'], min_visibility=0.3), additional_targets = {'rgb' : 'mask'})

        self.dataframe.img_shape = self.dataframe.img_shape.apply(eval)
        self.all_img_ids = np.unique(self.dataframe.image_id)
        self.small_img_ids = np.unique(self.dataframe[self.dataframe.img_shape == (500,500)].image_id)
        self.big_img_ids = np.unique(self.dataframe[self.dataframe.img_shape == (1000,1000)].image_id)

        self.grouped_df = self.dataframe.groupby('image_id')

    def __len__(self):
        return self.all_img_ids.shape[0]

    def __getitem__(self, idx):
        image_id = self.all_img_ids[idx]
        image_path = os.path.join(self.root, f'{image_id}.tif')
        image = cv2.imread(image_path)
        rgb = image.copy()

        group = self.grouped_df.get_group(image_id)
        if not group.bbox.isna().any():
            bboxes = group.bbox.apply(eval).to_list()
            categories = group.category_id.apply(int).to_list()
            filtered = [(item, label) for item, label in zip(bboxes, categories) if item[2] >= 5 and item[3] >= 5]
            bboxes, categories = zip(*filtered)
            bboxes = list(bboxes)
            categories = list(categories)
        else:
            bboxes = []
            categories = []

        if image_id in self.big_img_ids:
            augmented = self.transforms(image=image, bboxes=bboxes, categories=categories, rgb=rgb)

            image = augmented['image']
            bboxes = augmented['bboxes']
            categories = augmented['categories']
            rgb = augmented['rgb']

        elif image_id in self.small_img_ids:
            more_img_ids = self.small_img_ids[self.small_img_ids != image_id]
            more_img_ids = np.random.choice(more_img_ids, 3, replace=False)

            combined_img = np.zeros((1000, 1000, 3))
            combined_rgb = np.zeros((1000, 1000, 3))
            combined_bboxes = np.empty((0, 4))
            combined_categories = []
            
            augmented = self.transforms(image=image, bboxes=bboxes, categories=categories, rgb=rgb)
            combined_img[:500, :500] = augmented['image']
            combined_rgb[:500, :500] = augmented['rgb']
            if augmented['bboxes']:
                combined_bboxes = np.vstack([combined_bboxes, np.array(augmented['bboxes'])])
                combined_categories.extend(augmented['categories'])

            for i, image_id in enumerate(more_img_ids):
                image_path = os.path.join(self.root, f'{image_id}.tif')
                image = cv2.imread(image_path)
                rgb = image.copy()
                
                group = self.grouped_df.get_group(image_id)
                if not group.bbox.isna().any():
                    bboxes = group.bbox.apply(eval).to_list()
                    categories = group.category_id.apply(int).to_list()
                    filtered = [(item, label) for item, label in zip(bboxes, categories) if item[2] >= 5 and item[3] >= 5]
                    bboxes, categories = zip(*filtered)
                    bboxes = list(bboxes)
                    categories = list(categories)
                else:
                    bboxes = []
                    categories = []

                augmented = self.transforms(image=image, bboxes=bboxes, categories=categories, rgb=rgb)
                if i == 0:
                    combined_img[:500, 500:] = augmented['image']
                    combined_rgb[:500, 500:] = augmented['rgb']
                    if augmented['bboxes']:
                        new_bboxes = np.array(augmented['bboxes'])
                        new_bboxes[:, 0] += 500
                        combined_bboxes = np.vstack([combined_bboxes, new_bboxes])
                        combined_categories.extend(augmented['categories'])
                elif i == 1:
                    combined_img[500:, :500] = augmented['image']
                    combined_rgb[500:, :500] = augmented['rgb']
                    if augmented['bboxes']:
                        new_bboxes = np.array(augmented['bboxes'])
                        new_bboxes[:, 1] += 500
                        combined_bboxes = np.vstack([combined_bboxes, new_bboxes])
                        combined_categories.extend(augmented['categories'])
                elif i == 2:
                    combined_img[500:, 500:] = augmented['image']
                    combined_rgb[500:, 500:] = augmented['rgb']
                    if augmented['bboxes']:
                        new_bboxes = np.array(augmented['bboxes'])
                        new_bboxes[:, 0] += 500
                        new_bboxes[:, 1] += 500
                        combined_img[500:, 500:] = augmented['image']
                        combined_bboxes = np.vstack([combined_bboxes, new_bboxes])
                        combined_categories.extend(augmented['categories'])
                
            image = combined_img
            rgb = combined_rgb
            bboxes = combined_bboxes.tolist()
            categories = combined_categories

        cropped = self.crop_transform(image=image, bboxes=bboxes, categories=categories, rgb=rgb)
        image = cropped['image']
        rgb = cropped['rgb']
        bboxes = cropped['bboxes']
        categories = cropped['categories']

        # image = self.norm_transform(image=image)['image']
        image = image / 255
        image = torch.from_numpy(image).to(torch.float32).permute((2,0,1))
        rgb = rgb.astype(np.uint8)
        bboxes = np.array(bboxes) 

        if bboxes.shape[0]:
            bboxes[:, 2:4] = bboxes[:, 0:2] + bboxes[:, 2:4] 
            bboxes = bboxes[((bboxes[:, 2] - bboxes[:, 0]) >= 5) & ((bboxes[:, 3] - bboxes[:, 1]) >= 5)]

        if bboxes.shape[0]:
            area = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])
            target = {
                "boxes"  : torch.from_numpy(bboxes).to(torch.float32),
                "labels" : torch.as_tensor(categories).to(torch.int64),
                "area"   : torch.from_numpy(area).to(torch.float32),
                "iscrowd": torch.zeros((bboxes.shape[0],), dtype=torch.int64)
            }
        else:
            target = {
                "boxes"   : torch.zeros((0, 4), dtype=torch.float32),
                "labels"  : torch.zeros(0, dtype=torch.int64),
                "area"    : torch.zeros(0, dtype=torch.float32),
                "iscrowd" : torch.zeros((0,), dtype=torch.int64)
            }

        return image, target, rgb
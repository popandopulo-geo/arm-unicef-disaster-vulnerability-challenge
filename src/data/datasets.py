import os
import abc

import pandas as pd
import numpy as np
import cv2
from PIL import Image

import torch
from torch.utils.data import DataLoader, Dataset
from torch.utils.data.distributed import DistributedSampler
from torchvision.datasets import CocoDetection
from pycocotools.coco import COCO
import albumentations as A

from .utils import parse_coco_annotation, to_coco_annotation

class BaseDataset(Dataset):
    def __init__(self, **kwargs):
        self.root = kwargs.get("root")
        self.dataframe = kwargs.get("dataframe")
        self.phase = kwargs.get('phase', 'train')

    @abc.abstractmethod
    def __getitem__(self, idx):
        pass

    def __len__(self):
        return self.dataframe.shape[0]

    @classmethod
    def get_dataloader(cls, **kwargs):
        dataset = cls(**kwargs)

        if kwargs.get('cpu', False):
            _get_dataloader = cls.__get_dataloader_cpu
        elif int(os.environ["WORLD_SIZE"]) > 1: 
            _get_dataloader = cls.__get_dataloader_ddp
        else:
            _get_dataloader = cls.__get_dataloader

        return _get_dataloader(dataset, **kwargs)

    @staticmethod
    def __get_dataloader_cpu(dataset, **kwargs):
        dataloader = DataLoader(dataset,
                                batch_size=kwargs.get("batch_size", 1),
                                shuffle=kwargs.get("shuffle", False),
                                num_workers=1,
                                collate_fn=kwargs.get("collate_fn", None))
        
        return dataloader
    
    @staticmethod
    def __get_dataloader(dataset, **kwargs):
        dataloader = DataLoader(dataset,
                                batch_size=kwargs.get("batch_size", 1),
                                shuffle=kwargs.get("shuffle", False),
                                num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]),
                                collate_fn=kwargs.get("collate_fn", None))
        
        return dataloader
    
    @staticmethod
    def __get_dataloader_ddp(dataset, **kwargs):
        dataloader = DataLoader(dataset,
                                batch_size=kwargs.get("batch_size", 1),
                                shuffle=False,
                                sampler=DistributedSampler(train_dataset,
                                                            num_replicas=int(os.environ["WORLD_SIZE"]),
                                                            rank=int(os.environ["SLURM_PROCID"])),
                                num_workers=int(os.environ["SLURM_CPUS_PER_TASK"]),
                                collate_fn=kwargs.get("collate_fn", None))
        
        return dataloader


class RoofsDataset(BaseDataset):
    def __init__(self, **kwargs):
        super(RoofsDataset, self).__init__(**kwargs)

        self.transforms = kwargs.get("transforms", [])
        self.transforms = A.Compose(self.transforms, bbox_params=A.BboxParams(format='coco', label_fields=['categories'], min_visibility=0.3), additional_targets = {'rgb' : 'mask'})
        self.crop_transform = A.Compose([A.RandomCrop(height=500, width=500, p=1)], bbox_params=A.BboxParams(format='coco', label_fields=['categories'], min_visibility=0.3), additional_targets = {'rgb' : 'mask'})

        self.dataframe.img_shape = self.dataframe.img_shape.apply(eval)
        self.all_img_ids = np.unique(self.dataframe.image_id)
        self.small_img_ids = np.unique(self.dataframe[self.dataframe.img_shape == (500,500)].image_id)
        self.big_img_ids = np.unique(self.dataframe[self.dataframe.img_shape == (1000,1000)].image_id)

        self.grouped_df = self.dataframe.groupby('image_id')

    def __len__(self):
        return self.all_img_ids.shape[0]

    def __getitem__(self, idx):
        img_id = self.all_img_ids[idx]

        if self.phase == 'train':

            if img_id in self.big_img_ids:
                img, rgb, nd_mask, bboxes, categories = self.read_img(img_id)
                aug = self.transforms(image=img, bboxes=bboxes, categories=categories, rgb=rgb, mask=nd_mask)
                img, rgb, nd_mask, bboxes, categories = aug['image'], aug['rgb'], aug['mask'], aug['bboxes'], aug['categories']

            elif img_id in self.small_img_ids:
                additional_img_ids = self.small_img_ids[self.small_img_ids != img_id]
                additional_img_ids = np.random.choice(additional_img_ids, 3, replace=False).tolist()
                img, rgb, nd_mask, bboxes, categories = self.mosaic_aug([img_id] + additional_img_ids)

            return img, rgb, nd_mask, bboxes, categories

        else:

            img_path = os.path.join(self.root, f'{idx}.tif')
            img = cv2.imread(img_path)
            rgb = img.copy()
            nd_mask = self.get_no_data_mask(img)

            return img, rgb, nd_mask, None, None

    def read_img(self, img_id):
        img_path = os.path.join(self.root, f'{img_id}.tif')
        img = cv2.imread(img_path)
        rgb = img.copy()
        nd_mask = self.get_no_data_mask(img)
        
        group = self.grouped_df.get_group(img_id)
        bboxes, categories = self.get_bboxes_and_labels(group)

        return img, rgb, nd_mask, bboxes, categories

    def get_bboxes_and_labels(self, group):
        if group.bbox.isna().any():
            return [], []

        else:
            bboxes = group.bbox.apply(eval).to_list()
            categories = group.category_id.apply(int).to_list()

            bboxes, categories = zip(*[(b, c) for b, c in zip(bboxes, categories) if b[2] > 0 and b[3] > 0])
            return list(bboxes), list(categories)

    def mosaic_aug(self, img_ids):
        mosaic_img = np.zeros((1000, 1000, 3))
        mosaic_rgb = np.zeros((1000, 1000, 3))
        mosaic_nd_mask = np.zeros((1000, 1000))
        mosaic_bboxes = np.empty((0, 4))
        mosaic_categories = []

        for i, img_id in enumerate(img_ids):
            img, rgb, nd_mask, bboxes, categories = self.read_img(img_id)
            aug = self.transforms(image=img, bboxes=bboxes, categories=categories, rgb=rgb, mask=nd_mask)

            if i == 0:
                mosaic_img[:500, :500] = aug['image']
                mosaic_rgb[:500, :500] = aug['rgb']
                mosaic_nd_mask[:500, :500] = aug['mask']
                if aug['bboxes']:
                    mosaic_bboxes = np.vstack([mosaic_bboxes, np.array(aug['bboxes'])])
                    mosaic_categories.extend(aug['categories'])
            elif i == 1:
                mosaic_img[:500, 500:] = aug['image']
                mosaic_rgb[:500, 500:] = aug['rgb']
                mosaic_nd_mask[:500, 500:] = aug['mask']
                if aug['bboxes']:
                    new_bboxes = np.array(aug['bboxes'])
                    new_bboxes[:, 0] += 500
                    mosaic_bboxes = np.vstack([mosaic_bboxes, new_bboxes])
                    mosaic_categories.extend(aug['categories'])
            elif i == 2:
                mosaic_img[500:, :500] = aug['image']
                mosaic_rgb[500:, :500] = aug['rgb']
                mosaic_nd_mask[500:, :500] = aug['mask']
                if aug['bboxes']:
                    new_bboxes = np.array(aug['bboxes'])
                    new_bboxes[:, 1] += 500
                    mosaic_bboxes = np.vstack([mosaic_bboxes, new_bboxes])
                    mosaic_categories.extend(aug['categories'])
            elif i == 3:
                mosaic_img[500:, 500:] = aug['image']
                mosaic_rgb[500:, 500:] = aug['rgb']
                mosaic_nd_mask[500:, 500:] = aug['mask']
                if aug['bboxes']:
                    new_bboxes = np.array(aug['bboxes'])
                    new_bboxes[:, 0] += 500
                    new_bboxes[:, 1] += 500
                    mosaic_img[500:, 500:] = aug['image']
                    mosaic_bboxes = np.vstack([mosaic_bboxes, new_bboxes])
                    mosaic_categories.extend(aug['categories'])
            
        return mosaic_img, mosaic_rgb, mosaic_nd_mask, mosaic_bboxes.tolist(), mosaic_categories

    def get_no_data_mask(self, img, low_threshold=5, high_threshold=250):
        l = (img < low_threshold).all(axis=-1)
        h = (img > high_threshold).all(axis=-1)

        return (l | h).astype(np.uint8)

class RoofsDataset_FasterRCNN(RoofsDataset):
    def __init__(self, **kwargs):
        super(RoofsDataset_FasterRCNN, self).__init__(**kwargs)

    def __getitem__(self, idx):
        img, rgb, nd_mask, bboxes, categories = super(RoofsDataset_FasterRCNN, self).__getitem__(idx)

        img = img / 255.0
        img = torch.from_numpy(img).to(torch.float32).permute((2,0,1))
        rgb = rgb.astype(np.uint8)
        nd_mask = torch.from_numpy(nd_mask).to(torch.uint8)

        if self.phase == 'train':

            bboxes = np.array(bboxes)
            categories = np.array(categories)

            if bboxes.shape[0]:
                bboxes[:, 2:4] = bboxes[:, 0:2] + bboxes[:, 2:4] 
                categories = categories[((bboxes[:, 2] - bboxes[:, 0]) > 0) & ((bboxes[:, 3] - bboxes[:, 1]) > 0)]
                bboxes = bboxes[((bboxes[:, 2] - bboxes[:, 0]) > 0) & ((bboxes[:, 3] - bboxes[:, 1]) > 0)]

            if bboxes.shape[0]:
                area = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])
                target = {
                    "boxes"  : torch.from_numpy(bboxes).to(torch.float32),
                    "labels" : torch.from_numpy(categories).to(torch.int64),
                    "area"   : torch.from_numpy(area).to(torch.float32),
                    "iscrowd": torch.zeros((bboxes.shape[0],), dtype=torch.int64)
                }
            else:
                target = {
                    "boxes"   : torch.zeros((0, 4), dtype=torch.float32),
                    "labels"  : torch.zeros(0, dtype=torch.int64),
                    "area"    : torch.zeros(0, dtype=torch.float32),
                    "iscrowd" : torch.zeros((0,), dtype=torch.int64)
                }

            return img, target, rgb, nd_mask

        else:

            return img_id, img, rgb, nd_mask

class RoofsDataset_DETR(RoofsDataset):
    def __init__(self, **kwargs):
        super(RoofsDataset_DETR, self).__init__(**kwargs)
        self.processor = kwargs.get('processor') 

        self.norm_transform = A.Compose([
            A.Normalize(mean=kwargs.get("mean", (0.485, 0.456, 0.406)), 
                        std =kwargs.get("sdt" , (0.229, 0.224, 0.225)),
                        max_pixel_value=255.0)
        ])

    def __getitem__(self, idx):
        img, rgb, nd_mask, bboxes, categories = super(RoofsDataset_DETR, self).__getitem__(idx)

        if len(bboxes):
            bboxes = np.array(bboxes)
            categories = np.array(categories)
            # bboxes[:, 2:4] = bboxes[:, 0:2] + bboxes[:, 2:4] 
            categories = categories[(bboxes[:, 2] > 0) & (bboxes[:, 3] > 0)]
            bboxes = bboxes[(bboxes[:, 2] > 0) & (bboxes[:, 3] > 0)]
            bboxes = bboxes.tolist()
            categories = categories.tolist()

        annotation = to_coco_annotation(idx, bboxes, categories)
        img = Image.fromarray(np.uint8(img))
        rgb = torch.from_numpy(rgb.astype(np.uint8))
        nd_mask = torch.from_numpy(nd_mask).to(torch.uint8)

        if self.phase == 'train':

            target = {'image_id': idx, 'annotations': annotation}
            encoding = self.processor(images=img, annotations=target, return_tensors="pt")

            features = encoding["pixel_values"].squeeze()
            # features = torch.from_numpy(features)
            targets = encoding["labels"][0]

            return features, targets, rgb, nd_mask

        else:

            return img_id, img, rgb, nd_mask


class RoofsDataset_DETR_base(CocoDetection, BaseDataset):
    def __init__(self, **kwargs):
        super(RoofsDataset_DETR_base, self).__init__(kwargs.get("img_folder"), kwargs.get("ann_file"))
        self.processor = kwargs.get("processor")
        self.keep = kwargs.get('keep', [1,2,3])

    def __getitem__(self, idx):
        # read in PIL image and target in COCO format
        # feel free to add data augmentation here before passing them to the next step
        img, ann = super(RoofsDataset_DETR_base, self).__getitem__(idx)
        ann = list(filter(lambda x: x['category_id'] in self.keep, ann))
        rgb = np.array(img)

        # preprocess image and target (converting target to DETR format, resizing + normalization of both image and target)
        image_id = self.ids[idx]
        target = {'image_id': image_id, 'annotations': ann}
        encoding = self.processor(images=img, annotations=target, return_tensors="pt")

        features = encoding["pixel_values"].squeeze()
        target = encoding["labels"][0]
        rgb = cv2.resize(rgb, dsize=features.shape[1:], interpolation=cv2.INTER_CUBIC)

        return features, target, rgb
